{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDcwLoTdU7+T+lAKFGAX2T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"B3D9yq8RpgzS"},"outputs":[],"source":["# Image Classification with Convolutional Neural Networks (CNNs)\n","\n","---\n","\n","## Introduction\n","\n","**Image Classification** is the task of assigning a label (class) to an image based on its visual content. CNNs are the standard approach for image classification due to their ability to automatically extract **hierarchical features** from images, reducing the need for manual feature engineering.\n","\n","---\n","\n","## Key Concepts\n","\n","### 1. Input Layer\n","- Accepts raw image data.\n","- Typically shaped as `(height, width, channels)`; for grayscale images, channels = 1; for RGB images, channels = 3.\n","\n","### 2. Convolutional Layers\n","- Apply **filters/kernels** that slide over the image to extract local patterns such as edges, textures, and shapes.\n","- Multiple filters produce multiple feature maps.\n","- Example: Detecting vertical or horizontal edges in early layers.\n","\n","### 3. Activation Function\n","- Introduces **non-linearity**.\n","- ReLU (Rectified Linear Unit) is commonly used in CNNs.\n","\n","### 4. Pooling Layers\n","- Reduce spatial dimensions of feature maps to **lower computational cost** and **control overfitting**.\n","- Types:\n","  - **Max Pooling:** Takes the maximum value from a region.\n","  - **Average Pooling:** Takes the average value from a region.\n","\n","### 5. Fully Connected Layers\n","- Flatten feature maps into a 1D vector.\n","- Perform **high-level reasoning** to map features to classes.\n","\n","### 6. Output Layer\n","- Produces final class predictions.\n","- For multi-class classification, **softmax activation** is used to output probabilities for each class.\n","\n","---\n","\n","## Training Process\n","\n","1. **Forward Propagation:** Pass images through the network to compute predictions.\n","2. **Loss Calculation:** Compare predictions with true labels using a loss function (e.g., categorical crossentropy).\n","3. **Backpropagation:** Compute gradients of the loss with respect to network parameters.\n","4. **Weight Update:** Update weights using an optimizer (e.g., Adam, SGD).\n","5. Repeat for multiple **epochs** until the network learns to classify images accurately.\n","\n","---\n","\n","## Applications\n","\n","- Handwritten digit recognition (MNIST)\n","- Object detection and recognition (CIFAR-10, ImageNet)\n","- Medical imaging (tumor detection, X-ray classification)\n","- Facial recognition\n","- Autonomous vehicles (traffic sign recognition)\n","\n","---\n","\n","## Advantages of CNNs for Image Classification\n","\n","- Automatically **learn spatial features** without manual feature engineering.\n","- **Parameter sharing** reduces the number of parameters compared to fully connected networks.\n","- **Hierarchical learning:** Early layers detect simple patterns; deeper layers detect complex structures.\n","- Works well on large-scale image datasets.\n"]},{"cell_type":"code","source":["# ==============================\n","# Image Classification with CNNs - CIFAR-10 Example\n","# ==============================\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","\n","# 1. Load dataset (CIFAR-10)\n","(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","\n","# 2. Preprocess data\n","X_train = X_train.astype('float32') / 255.0\n","X_test = X_test.astype('float32') / 255.0\n","\n","# One-hot encode labels\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","\n","# 3. Build CNN model\n","model = Sequential([\n","    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n","    MaxPooling2D(pool_size=(2,2)),\n","    Conv2D(64, (3,3), activation='relu'),\n","    MaxPooling2D(pool_size=(2,2)),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.5),\n","    Dense(10, activation='softmax')\n","])\n","\n","# 4. Compile the model\n","model.compile(\n","    optimizer='adam',\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# 5. Train the model\n","history = model.fit(\n","    X_train, y_train,\n","    validation_split=0.2,\n","    epochs=20,\n","    batch_size=64,\n","    verbose=1\n",")\n","\n","# 6. Evaluate the model\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"\\nTest Loss: {loss:.4f}\")\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","# 7. Plot training history\n","plt.figure(figsize=(10,5))\n","plt.plot(history.history['accuracy'], label='Train Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.title('CNN Accuracy over Epochs')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","plt.figure(figsize=(10,5))\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('CNN Loss over Epochs')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7DkUhADpvUt","outputId":"d5d412ea-a46f-4fb3-9049-19e7d09d95e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.3073 - loss: 1.8732 - val_accuracy: 0.5163 - val_loss: 1.3687\n","Epoch 2/20\n","\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4959 - loss: 1.3939"]}]}]}