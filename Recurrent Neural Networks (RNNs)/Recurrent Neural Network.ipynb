{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3ea32capa6ymvgyGZldvO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Recurrent Neural Networks (RNNs) and LSTM\n","\n","---\n","\n","## RNN Basics and Applications\n","\n","**Definition:**  \n","A **Recurrent Neural Network (RNN)** is a type of neural network designed to process sequential data by maintaining a memory of previous inputs. Unlike traditional feedforward networks, RNNs have loops that allow information to persist across time steps.\n","\n","**Key Features:**\n","- Handles sequences of variable length (e.g., text, time series).  \n","- Maintains hidden states to capture temporal dependencies.  \n","\n","**Applications:**\n","- Time series prediction (e.g., stock prices, weather forecasting)  \n","- Natural language processing (e.g., language modeling, text generation)  \n","- Speech recognition  \n","- Anomaly detection in sequences  \n","\n","**Limitations:**\n","- Struggles with **long-term dependencies** due to vanishing or exploding gradients.\n","\n","---\n","\n","## Long Short-Term Memory (LSTM) Networks\n","\n","**Definition:**  \n","**LSTM** is a type of RNN designed to overcome the limitations of standard RNNs. LSTM networks introduce **gates** to control the flow of information, making it easier to learn long-term dependencies.\n","\n","**Key Components of LSTM:**\n","1. **Forget Gate:** Decides which information to discard from the cell state.  \n","2. **Input Gate:** Decides which new information to store in the cell state.  \n","3. **Cell State:** Acts as a memory that carries relevant information across time steps.  \n","4. **Output Gate:** Determines the output based on the cell state.\n","\n","**Applications:**\n","- Sequence prediction  \n","- Language translation  \n","- Sentiment analysis  \n","- Speech recognition  \n","\n","---\n","\n","## Time Series Forecasting with RNNs\n","\n","**Definition:**  \n","Time series forecasting involves predicting future values based on previously observed sequential data. RNNs and LSTMs are particularly effective because they can capture temporal dependencies in sequential datasets.\n","\n","**Key Steps:**\n","1. Preprocess the time series data (normalization, windowing).  \n","2. Use RNN/LSTM layers to learn temporal patterns.  \n","3. Train the model on historical data.  \n","4. Predict future values.\n","\n","**Applications:**\n","- Stock market prediction  \n","- Weather and climate forecasting  \n","- Energy demand prediction  \n","- Sales and inventory forecasting  \n","\n","**Advantages of LSTM over RNN:**\n","- Handles long-term dependencies  \n","- Avoids vanishing/exploding gradient problem  \n","\n","---\n","\n","## NLP with LSTM & GRU\n","\n","**Definition:**  \n","LSTM and GRU (Gated Recurrent Unit) networks are widely used in **Natural Language Processing (NLP)** tasks because they can capture sequential dependencies in text.\n","\n","**Key Concepts:**\n","- **GRU:** A simplified version of LSTM with fewer gates (reset and update), often faster to train with comparable performance.  \n","- Both LSTM and GRU can remember context in sequences, making them suitable for tasks requiring understanding of word order and context.\n","\n","**Applications in NLP:**\n","- Text generation and summarization  \n","- Language translation  \n","- Sentiment analysis  \n","- Named Entity Recognition (NER)  \n","\n","**Comparison:**\n","| Feature | LSTM | GRU |\n","|---------|------|-----|\n","| Gates | 3 (Input, Forget, Output) | 2 (Update, Reset) |\n","| Memory | Separate cell state | Combined hidden state |\n","| Training Speed | Slower | Faster |\n","| Performance | Often slightly better | Competitive for smaller datasets |\n"],"metadata":{"id":"rYSadrc8wINx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7gQR6_pvTtH"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import SimpleRNN, Dense\n","\n","# Example sequence data: y = sum of previous two numbers\n","X = np.array([[0,1],[1,2],[2,3],[3,4],[4,5]], dtype=float)\n","y = np.array([1,3,5,7,9], dtype=float)\n","\n","# Reshape input for RNN [samples, time_steps, features]\n","X = X.reshape((X.shape[0], X.shape[1], 1))\n","\n","# Build RNN model\n","model = Sequential()\n","model.add(SimpleRNN(10, activation='relu', input_shape=(X.shape[1], 1)))\n","model.add(Dense(1))\n","model.compile(optimizer='adam', loss='mse')\n","\n","# Train\n","model.fit(X, y, epochs=200, verbose=0)\n","\n","# Predict\n","test_input = np.array([5,6]).reshape((1,2,1))\n","print(\"Prediction:\", model.predict(test_input))\n"]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# Sample corpus\n","corpus = [\n","    \"I love machine learning\",\n","    \"I love deep learning\",\n","    \"I enjoy natural language processing\",\n","    \"Deep learning is amazing\"\n","]\n","\n","# Tokenize text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Create input sequences\n","input_sequences = []\n","for line in corpus:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# Pad sequences\n","max_seq_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n","\n","# Split into predictors and label\n","X = input_sequences[:, :-1]\n","y = input_sequences[:, -1]\n","\n","# One-hot encode output\n","from tensorflow.keras.utils import to_categorical\n","y = to_categorical(y, num_classes=total_words)\n","\n","# Build LSTM model\n","model = Sequential()\n","model.add(Embedding(total_words, 10, input_length=max_seq_len-1))\n","model.add(LSTM(50))\n","model.add(Dense(total_words, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","# Train\n","model.fit(X, y, epochs=300, verbose=0)\n","\n","# Predict next word\n","seed_text = \"I love\"\n","token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n","predicted = model.predict(token_list, verbose=0)\n","predicted_word = tokenizer.index_word[np.argmax(predicted)]\n","print(\"Next word prediction:\", predicted_word)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjmvGaGuwdYM","executionInfo":{"status":"ok","timestamp":1759842098484,"user_tz":-330,"elapsed":21353,"user":{"displayName":"Kartik Rajesh Patel","userId":"13704250535376199674"}},"outputId":"22c636eb-9e93-46e0-81e3-56320bdc958f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Next word prediction: deep\n"]}]}]}