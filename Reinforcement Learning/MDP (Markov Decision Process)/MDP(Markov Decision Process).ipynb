{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7BJ7Rh/dcbzaTDaIEiPVo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Markov Decision Processes (MDP)\n","\n","---\n","\n","##  Theory\n","\n","A **Markov Decision Process (MDP)** is a mathematical framework used for **modeling decision-making problems** in which outcomes are partly random and partly under the control of a decision-maker (agent).  \n","It forms the **foundation of Reinforcement Learning (RL)**.\n","\n","---\n","\n","## Components of an MDP\n","\n","An MDP is defined by the tuple:\n","\n","\\[\n","(S, A, P, R, \\gamma)\n","\\]\n","\n","| Symbol | Name | Description |\n","|:--------|:------|:-------------|\n","| **S** | States | The set of all possible situations the agent can be in. |\n","| **A** | Actions | The set of all possible actions the agent can take. |\n","| **P** | Transition Probability | The probability \\( P(s'|s,a) \\) of moving to state *s’* from state *s* after taking action *a*. |\n","| **R** | Reward Function | The immediate reward received after taking action *a* in state *s*. |\n","| **γ** | Discount Factor | Determines how much future rewards are valued compared to immediate rewards (0 ≤ γ ≤ 1). |\n","\n","---\n","\n","##  Example\n","\n","Imagine a robot navigating a 3x3 grid:\n","\n","- **States (S):** Positions in the grid.  \n","- **Actions (A):** {Up, Down, Left, Right}.  \n","- **Rewards (R):** +10 for reaching the goal, -10 for falling into a trap, and 0 otherwise.  \n","- **Transition Probability (P):** If the robot moves up, there’s a 90% chance it moves correctly, and 10% chance it slips sideways.  \n","- **Discount Factor (γ):** 0.9 (to favor immediate rewards slightly more).\n","\n","---\n","\n","##  Objective\n","\n","The goal of an agent in an MDP is to find an **optimal policy (π\\*)** that maximizes the **expected cumulative reward** over time:\n","\n","\\[\n","V^{π}(s) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\,|\\, s_0 = s \\right]\n","\\]\n","\n","where:\n","- \\( V^{π}(s) \\): value function (expected return starting from state *s* under policy *π*)  \n","- \\( R_t \\): reward at time step *t*  \n","- \\( \\gamma \\): discount factor\n","\n","---\n","\n","##  Solution Approaches\n","\n","1. **Value Iteration** – Iteratively update value functions until convergence.  \n","2. **Policy Iteration** – Alternate between evaluating and improving a policy.  \n","3. **Q-Learning / SARSA** – Model-free approaches used in Reinforcement Learning.\n","\n","---\n","\n","## Applications\n","\n","- Robotics navigation  \n","- Game AI (e.g., chess, Go, gridworld)  \n","- Autonomous vehicles  \n","- Finance & inventory optimization  \n","- Dialogue systems\n","\n","---\n","\n"],"metadata":{"id":"yNU92hwikV-u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0SqM-vzkSQB"},"outputs":[],"source":["# ==============================\n","# Markov Decision Process (MDP) - Value Iteration Example\n","# ==============================\n","\n","import numpy as np\n","\n","# 1. Define MDP Components\n","states = [\"A\", \"B\", \"C\", \"D\"]  # 4 states\n","actions = [\"left\", \"right\"]\n","\n","# Transition probabilities and rewards\n","# P[s][a] = [(prob, next_state, reward, done)]\n","P = {\n","    \"A\": {\n","        \"right\": [(1.0, \"B\", 0, False)],\n","        \"left\": [(1.0, \"A\", 0, False)]\n","    },\n","    \"B\": {\n","        \"right\": [(1.0, \"C\", 0, False)],\n","        \"left\": [(1.0, \"A\", 0, False)]\n","    },\n","    \"C\": {\n","        \"right\": [(1.0, \"D\", 1, True)],   # Reaching D gives reward +1\n","        \"left\": [(1.0, \"B\", 0, False)]\n","    },\n","    \"D\": {\n","        \"right\": [(1.0, \"D\", 0, True)],\n","        \"left\": [(1.0, \"D\", 0, True)]\n","    }\n","}\n","\n","# 2. Initialize parameters\n","gamma = 0.9  # discount factor\n","theta = 1e-4  # convergence threshold\n","V = {s: 0 for s in states}  # initial value function\n","\n","# 3. Value Iteration Algorithm\n","iteration = 0\n","while True:\n","    delta = 0\n","    iteration += 1\n","    print(f\"\\nIteration {iteration}\")\n","\n","    for s in states:\n","        v = V[s]\n","        action_values = []\n","\n","        for a in actions:\n","            val = 0\n","            for prob, next_state, reward, done in P[s][a]:\n","                val += prob * (reward + gamma * V[next_state])\n","            action_values.append(val)\n","\n","        V[s] = max(action_values)\n","        delta = max(delta, abs(v - V[s]))\n","        print(f\"State {s}: Value = {V[s]:.4f}\")\n","\n","    if delta < theta:\n","        break\n","\n","# 4. Derive the Optimal Policy\n","policy = {}\n","for s in states:\n","    best_action = None\n","    best_value = float('-inf')\n","    for a in actions:\n","        val = sum([prob * (reward + gamma * V[next_state]) for prob, next_state, reward, done in P[s][a]])\n","        if val > best_value:\n","            best_value = val\n","            best_action = a\n","    policy[s] = best_action\n","\n","# 5. Display Results\n","print(\"\\nFinal Value Function:\")\n","for s in V:\n","    print(f\"{s}: {V[s]:.4f}\")\n","\n","print(\"\\nOptimal Policy:\")\n","for s in policy:\n","    print(f\"{s}: {policy[s]}\")\n"]}]}