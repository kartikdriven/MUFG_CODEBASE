{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+uTAHLFnuCb0OpRWwOH+W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Q-Learning Algorithm\n","\n","---\n","\n","## Theory\n","\n","Q-Learning is a **model-free Reinforcement Learning algorithm** used to find the **optimal action-value function (Q-function)** that helps an agent make decisions to maximize long-term rewards.\n","\n","Unlike Markov Decision Processes (MDPs), Q-Learning does not require prior knowledge of the environment’s transition probabilities. It learns purely from experience through trial and error.\n","\n","---\n","\n","## Key Concept\n","\n","The algorithm learns the **Q-value** for each state-action pair, which represents the expected future reward of taking a specific action in a given state and then following the optimal policy thereafter.\n","\n","The **Q-value update rule** is given by:\n","\n","\\[\n","Q(s, a) = Q(s, a) + \\alpha \\times [R + \\gamma \\times \\max_{a'} Q(s', a') - Q(s, a)]\n","\\]\n","\n","Where:\n","\n","| Symbol | Meaning |\n","|:--------|:--------|\n","| **s** | Current state |\n","| **a** | Current action |\n","| **R** | Reward received after taking action *a* in state *s* |\n","| **s′** | Next state |\n","| **α** | Learning rate (how much new information overrides old) |\n","| **γ** | Discount factor (importance of future rewards) |\n","\n","---\n","\n","## Steps of the Algorithm\n","\n","1. Initialize the Q-table with zeros for all state-action pairs.  \n","2. For each episode:\n","   - Choose an action **a** using an exploration strategy (like ε-greedy).  \n","   - Perform the action, observe the **reward (R)** and **next state (s′)**.  \n","   - Update the Q-value using the update rule.  \n","   - Move to the next state.  \n","3. Repeat until convergence — when Q-values stabilize or the maximum number of episodes is reached.  \n","\n","---\n","\n","## Exploration vs Exploitation\n","\n","- **Exploration:** Take random actions to discover rewards.  \n","- **Exploitation:** Take the best-known action according to current Q-values.  \n","- Typically managed using an **ε-greedy policy**, where ε is the probability of exploring.\n","\n","---\n","\n","## Goal\n","\n","To learn the **optimal policy (π\\*)**, which maximizes the **expected cumulative reward** over time:\n","\n","\\[\n","\\pi^*(s) = \\arg\\max_a Q(s, a)\n","\\]\n","\n","Once the Q-table converges, the agent can select the **best action for each state** to maximize rewards.\n","\n","---\n","\n","## Applications\n","\n","- Robotics navigation and control  \n","- Game AI (chess, tic-tac-toe, gridworld)  \n","- Autonomous vehicles  \n","- Inventory and supply chain optimization  \n","- Finance and trading  \n","\n","---\n","\n","## Advantages\n","\n","- Simple and effective for **discrete state-action spaces**  \n","- **Model-free**: no need for environment probabilities  \n","- Can converge to **optimal policy** with enough exploration  \n","\n","---\n","\n","## Limitations\n","\n","- Does not scale well to **large or continuous state spaces**  \n","- Convergence can be slow without proper **hyperparameters**  \n","- Requires **exploration strategy** to avoid local optima\n"],"metadata":{"id":"v0whAaFElVqX"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qohQ73u-lUqb","executionInfo":{"status":"ok","timestamp":1759637817101,"user_tz":-330,"elapsed":1437,"user":{"displayName":"Kartik Rajesh Patel","userId":"13704250535376199674"}},"outputId":"51442a96-9b94-4561-d5c4-274bec105c22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final Q-Table:\n","State 0: [0.59031642 0.6561    ]\n","State 1: [0.58576608 0.729     ]\n","State 2: [0.6560916 0.81     ]\n","State 3: [0.72895318 0.9       ]\n","State 4: [0.80999992 1.        ]\n","State 5: [0. 0.]\n","\n","Optimal Policy:\n","State 0: Right\n","State 1: Right\n","State 2: Right\n","State 3: Right\n","State 4: Right\n","State 5: Left\n"]}],"source":["# ==============================\n","# Q-Learning Algorithm - Simple GridWorld Example\n","# ==============================\n","\n","import numpy as np\n","import random\n","\n","# 1. Define environment (GridWorld)\n","# States: 0 to 5, Goal state = 5\n","n_states = 6\n","actions = [0, 1]  # 0 = left, 1 = right\n","goal_state = 5\n","\n","# Rewards: only goal state gives reward 1\n","R = np.zeros(n_states)\n","R[goal_state] = 1\n","\n","# Q-table initialization\n","Q = np.zeros((n_states, len(actions)))\n","\n","# Hyperparameters\n","alpha = 0.8       # learning rate\n","gamma = 0.9       # discount factor\n","epsilon = 0.2     # exploration probability\n","episodes = 50\n","\n","# 2. Q-Learning Algorithm\n","for ep in range(episodes):\n","    state = 0  # start state\n","    done = False\n","\n","    while not done:\n","        # Choose action (epsilon-greedy)\n","        if random.uniform(0, 1) < epsilon:\n","            action = random.choice(actions)  # explore\n","        else:\n","            action = np.argmax(Q[state, :])  # exploit\n","\n","        # Take action\n","        if action == 0:\n","            next_state = max(0, state - 1)  # move left\n","        else:\n","            next_state = min(n_states - 1, state + 1)  # move right\n","\n","        reward = R[next_state]\n","        done = (next_state == goal_state)\n","\n","        # Update Q-value\n","        Q[state, action] = Q[state, action] + alpha * (\n","            reward + gamma * np.max(Q[next_state, :]) - Q[state, action]\n","        )\n","\n","        # Move to next state\n","        state = next_state\n","\n","# 3. Display results\n","print(\"Final Q-Table:\")\n","for s in range(n_states):\n","    print(f\"State {s}: {Q[s]}\")\n","\n","# 4. Derive Optimal Policy\n","policy = []\n","for s in range(n_states):\n","    best_action = np.argmax(Q[s])\n","    policy.append(\"Right\" if best_action == 1 else \"Left\")\n","\n","print(\"\\nOptimal Policy:\")\n","for s in range(n_states):\n","    print(f\"State {s}: {policy[s]}\")\n"]}]}