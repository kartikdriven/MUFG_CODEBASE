{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDE7aIeE1UMdcGHX8TcZuA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction to Neural Networks\n","\n","---\n","\n","## 1. Neural Networks Overview\n","\n","A **Neural Network (NN)** is a computational model inspired by the human brain, designed to recognize patterns and solve complex tasks such as classification, regression, and function approximation. Neural networks consist of layers of interconnected nodes (neurons), where each node performs a simple computation.\n","\n","**Key Components:**\n","- **Input layer:** Receives the features of the data.  \n","- **Hidden layers:** Intermediate layers that perform transformations on inputs.  \n","- **Output layer:** Produces the final prediction or classification.\n","\n","---\n","\n","## 2. Perceptrons\n","\n","A **Perceptron** is the simplest type of neural network, representing a single neuron. It performs a **weighted sum of inputs** followed by an **activation function**.\n","\n","Mathematical representation:\n","\n","\\[\n","y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n","\\]\n","\n","Where:  \n","- \\(x_i\\) = input features  \n","- \\(w_i\\) = weights  \n","- \\(b\\) = bias  \n","- \\(f\\) = activation function  \n","- \\(y\\) = output  \n","\n","**Perceptrons** are mainly used for **binary classification tasks**.  \n","\n","---\n","\n","## 3. Activation Functions\n","\n","Activation functions introduce **non-linearity** in the network, allowing it to learn complex patterns.\n","\n","| Function | Formula | Range | Use Case |\n","|----------|---------|-------|----------|\n","| **Step** | \\(f(x) = 1 \\text{ if } x \\ge 0 \\text{ else } 0\\) | 0 or 1 | Early perceptrons, binary classification |\n","| **Sigmoid** | \\(f(x) = \\frac{1}{1+e^{-x}}\\) | 0 to 1 | Probabilities, output layer for binary classification |\n","| **Tanh** | \\(f(x) = \\tanh(x)\\) | -1 to 1 | Hidden layers, zero-centered output |\n","| **ReLU** | \\(f(x) = \\max(0, x)\\) | 0 to ∞ | Hidden layers, efficient training |\n","| **Leaky ReLU** | \\(f(x) = x \\text{ if } x>0 \\text{ else } 0.01x\\) | -∞ to ∞ | Avoids dying ReLU problem |\n","\n","---\n","\n","## 4. Loss Functions\n","\n","Loss functions measure how well a neural network's predictions match the actual target values. Minimizing the loss guides the network to learn optimal weights.\n","\n","**Common Loss Functions:**\n","\n","| Loss Function | Formula | Use Case |\n","|---------------|--------|----------|\n","| **Mean Squared Error (MSE)** | \\( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\) | Regression tasks |\n","| **Mean Absolute Error (MAE)** | \\( \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\) | Regression, robust to outliers |\n","| **Binary Cross-Entropy** | \\( -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)] \\) | Binary classification |\n","| **Categorical Cross-Entropy** | \\( -\\sum_{i} y_i \\log(\\hat{y}_i) \\) | Multi-class classification |\n","\n","---\n","\n","## 5. Backpropagation\n","\n","**Backpropagation** is the algorithm used to **train neural networks** by updating weights to minimize the loss function. It involves two main steps:\n","\n","1. **Forward Pass:** Compute outputs of the network layer by layer and calculate the loss.  \n","2. **Backward Pass:** Propagate the error backward through the network using the **chain rule of calculus** to compute gradients of the loss with respect to each weight.  \n","\n","Weights are updated using **gradient descent**:\n","\n","\\[\n","w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n","\\]\n","\n","Where:  \n","- \\(w\\) = weight  \n","- \\(\\eta\\) = learning rate  \n","- \\(L\\) = loss function  \n","\n","---\n","\n","### Summary\n","\n","Neural Networks are powerful models that can approximate complex functions.  \n","- **Perceptrons** form the building blocks of neural networks.  \n","- **Activation functions** introduce non-linearity.  \n","- **Loss functions** quantify errors.  \n","- **Backpropagation** with gradient descent allows the network to learn from data.  \n"],"metadata":{"id":"tEFKtStEmSIc"}}]}